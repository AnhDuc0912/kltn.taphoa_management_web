"""
Build FAISS index from all sku_images in database
Based on Copy_of_finetune_ResNet_101.ipynb workflow
"""
import os
import sys
import json
from pathlib import Path

import faiss
import numpy as np
import psycopg2
import torch
from PIL import Image
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent))

from services.db_utils import q, exec_sql
from services.resnet101 import RetrievalNet, load_model, collate_fn_filter_none

# Constants
EMB_DIM = 512
BATCH_SIZE = 128
UPLOAD_DIR = os.getenv("UPLOAD_DIR", "uploads")  # th∆∞ m·ª•c uploads tr√™n server
UPLOAD_URL_PREFIX = os.getenv("UPLOAD_URL_PREFIX", "/uploads")  # prefix URL ƒë·ªÉ tr·∫£ ra client


class DatabaseImagesDataset(Dataset):
    """Dataset for loading images from database"""

    def __init__(self, image_records, upload_dir, transform=None):
        """
        Args:
            image_records: List of (id, sku_id, image_path) tuples
            upload_dir: Root directory for images
            transform: Image transforms
        """
        self.records = image_records
        self.upload_dir = upload_dir
        self.transform = transform

    def __len__(self):
        return len(self.records)

    def __getitem__(self, idx):
        img_id, sku_id, image_path = self.records[idx]

        # image_path trong DB c√≥ th·ªÉ l√† full path ho·∫∑c relative
        # Chu·∫©n ho√° l·∫°i: ch·ªâ l·∫•y t√™n file r·ªìi join v·ªõi UPLOAD_DIR
        norm_path = image_path.replace("\\", "/")
        filename = norm_path.split("/")[-1]
        full_path = os.path.join(self.upload_dir, filename)

        try:
            image = Image.open(full_path).convert("RGB")
            if self.transform:
                image = self.transform(image)
            return image, (img_id, sku_id, filename)
        except Exception as e:
            print(f"Error loading {full_path}: {e}")
            return None, (img_id, sku_id, filename)


def build_faiss_index_from_db(checkpoint_path="out-resnet101-model/finetuned_resnet101.pt"):
    """
    Build FAISS index t·ª´ vector image_vec trong DB.
    Ch·ªâ d√πng vector h·ª£p l·ªá (ƒë√∫ng 512 chi·ªÅu) v√† l∆∞u v√†o b·∫£ng faiss_indexes.
    """

    print("=== Build FAISS index t·ª´ image_vec trong DB ===")

    rows = q("""
        SELECT id, sku_id, image_path, image_vec
        FROM sku_images
        WHERE image_vec IS NOT NULL
        ORDER BY sku_id, is_primary DESC, id
    """)

    if not rows:
        print("‚ùå Kh√¥ng c√≥ image_vec trong DB.")
        return None

    vectors = []
    metadata_list = []
    EXPECTED_DIM = 512   # s·ªë chi·ªÅu vector mong ƒë·ª£i

    for img_id, sku_id, image_path, image_vec in rows:
        # print("DEBUG type:", img_id, type(image_vec))

        vec = None

        # 1) image_vec l√† string (case c·ªßa m√†y)
        if isinstance(image_vec, str):
            s = image_vec.strip()
            try:
                # N·∫øu l√† JSON thu·∫ßn: "[0.1, 0.2, ...]"
                vec_list = json.loads(s)
            except Exception:
                # N·∫øu l√† format kh√°c: "{0.1,0.2,...}" ho·∫∑c "(0.1,0.2,...)"
                for ch in "[](){}":
                    s = s.replace(ch, "")
                parts = [p for p in s.split(",") if p.strip()]
                try:
                    vec_list = [float(p) for p in parts]
                except Exception as e:
                    print(f"‚ö†Ô∏è Skip: kh√¥ng parse ƒë∆∞·ª£c image_vec string (id={img_id}): {e}")
                    continue

            vec = np.asarray(vec_list, dtype="float32")

        # 2) list / tuple
        elif isinstance(image_vec, (list, tuple)):
            vec = np.asarray(image_vec, dtype="float32")

        # 3) numpy array
        elif isinstance(image_vec, np.ndarray):
            vec = image_vec.astype("float32")

        # 4) bytes / memoryview (bytea)
        elif isinstance(image_vec, (bytes, bytearray, memoryview)):
            buf = bytes(image_vec)
            vec = np.frombuffer(buf, dtype="float32")

        else:
            print(f"‚ö†Ô∏è Skip: image_vec format kh√¥ng h·ªó tr·ª£ (id={img_id}, type={type(image_vec)})")
            continue

        # 5) Ki·ªÉm tra vector sau khi convert
        if vec is None or vec.size == 0:
            print(f"‚ö†Ô∏è Skip: vector r·ªóng ho·∫∑c None (id={img_id})")
            continue

        vec = vec.reshape(-1)

        if vec.shape[0] != EXPECTED_DIM:
            print(f"‚ö†Ô∏è Skip: vector size {vec.shape} != {EXPECTED_DIM} (id={img_id})")
            continue

        # vector OK ‚Üí l∆∞u l·∫°i
        vectors.append(vec)

        # chu·∫©n ho√° filename
        filename = image_path.replace("\\", "/").split("/")[-1]
        metadata_list.append((img_id, sku_id, filename))

    if not vectors:
        print("‚ùå Kh√¥ng c√≥ vector h·ª£p l·ªá n√†o ƒë·ªÉ build FAISS index")
        return None

    all_embeddings = np.vstack(vectors).astype("float32")

    print(f"‚û° T·ªïng vector h·ª£p l·ªá: {len(vectors)}")
    print(f"‚û° Embedding shape: {all_embeddings.shape}")

    # Build FAISS index
    dim = all_embeddings.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(all_embeddings)

    print(f"‚û° FAISS index size: {index.ntotal} items")

    # Serialize index + metadata ‚Üí L∆ØU V√ÄO DB
    serialized = faiss.serialize_index(index)
    metadata_json = json.dumps([list(m) for m in metadata_list])

    exec_sql("""
        INSERT INTO faiss_indexes (name, index_data, index_type, metadata)
        VALUES (%s, %s, 'IndexFlatL2', %s)
    """, ("resnet101_faiss", psycopg2.Binary(serialized), metadata_json))

    print("‚úÖ Saved FAISS index to DB.")

    return index, metadata_list

def search_image_with_faiss(query_image_path, k=5):
    """
    Search FAISS index l∆∞u trong b·∫£ng faiss_indexes.
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = load_model(device=device)
    model.eval()

    row = q("""
        SELECT index_data, metadata
        FROM faiss_indexes
        ORDER BY id DESC
        LIMIT 1
    """, fetch="one")

    if not row:
        print("‚ùå No FAISS index found in database. Run build_faiss_index_from_db() first.")
        return []

    serialized_index, metadata_json = row

    # üîß deserialize index t·ª´ bytes
    index_arr = np.frombuffer(serialized_index, dtype="uint8")
    index = faiss.deserialize_index(index_arr)

    # üîß metadata c√≥ th·ªÉ l√† string JSON ho·∫∑c list (jsonb)
    if isinstance(metadata_json, (str, bytes, bytearray)):
        metadata_list = json.loads(metadata_json)
    else:
        metadata_list = metadata_json

    print(f"Loaded FAISS index with {index.ntotal} embeddings")

    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(
            [0.485, 0.456, 0.406],
            [0.229, 0.224, 0.225],
        ),
    ])

    try:
        query_image = Image.open(query_image_path).convert("RGB")
        transformed_image = transform(query_image).unsqueeze(0).to(device)
    except Exception as e:
        print(f"‚ùå Error loading query image: {e}")
        return []

    with torch.no_grad():
        query_embedding = model(transformed_image).cpu().numpy().astype("float32")

    distances, indices = index.search(query_embedding, k)

    print(f"Found {k} nearest neighbors:")

    results = []
    for i in range(k):
        idx = indices[0][i]
        distance = distances[0][i]

        if 0 <= idx < len(metadata_list):
            img_id, sku_id, filename = metadata_list[idx]
            image_url = f"{UPLOAD_URL_PREFIX.rstrip('/')}/{filename}"

            print(f"Rank {i+1}: SKU {sku_id} | {filename} | Distance: {distance:.4f}")

            results.append({
                "rank": i + 1,
                "img_id": img_id,
                "sku_id": sku_id,
                "image_filename": filename,
                "image_url": image_url,
                "distance": float(distance),
                "score": 1.0 / (1.0 + distance),
            })

    return results

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description="Build FAISS index or search images"
    )
    parser.add_argument(
        "--build", action="store_true", help="Build FAISS index from database"
    )
    parser.add_argument(
        "--search", type=str, help="Path to query image for search"
    )
    parser.add_argument(
        "--k", type=int, default=5, help="Number of results to return"
    )
    parser.add_argument(
        "--checkpoint",
        type=str,
        default="out-resnet101-model/finetuned_resnet101.pt",
        help="Path to model checkpoint",
    )
    args = parser.parse_args()

    if args.build:
        print("Building FAISS index...")
        build_faiss_index_from_db(checkpoint_path=args.checkpoint)
    elif args.search:
        print(f"Searching for similar images to: {args.search}")
        results = search_image_with_faiss(args.search, k=args.k)
        if results:
            print(f"\n‚úÖ Found {len(results)} similar images")
            for r in results:
                print(r)
    else:
        parser.print_help()
