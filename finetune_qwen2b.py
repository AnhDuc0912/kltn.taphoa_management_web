# -*- coding: utf-8 -*-
"""finetune_qwen2B.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Db1l6ma6sKyLXpYCR7Go0hRLXRjm4Njs
"""

# Cài đặt thư viện với sửa lỗi
!pip cache purge
!pip install transformers==4.45.1 peft==0.12.0 trl==0.11.1 datasets==2.21.0 accelerate qwen-vl-utils bitsandbytes --force-reinstall
!pip install triton --upgrade  # Cài triton để hỗ trợ bitsandbytes

# Kiểm tra thư viện
import transformers, peft, trl, datasets, accelerate, bitsandbytes
print(f"transformers: {transformers.__version__}")
print(f"peft: {peft.__version__}")
print(f"trl: {trl.__version__}")
print(f"datasets: {datasets.__version__}")
print(f"accelerate: {accelerate.__version__}")
print(f"bitsandbytes: {bitsandbytes.__version__}")

import torch
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig
from transformers.trainer_utils import EvalPrediction
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import Dataset
from PIL import Image
import os
import json
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

import os, json, copy, math, random
from PIL import Image
import torch
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# ======= SỬA 2 ĐƯỜNG DẪN NÀY CHO PHÙ HỢP =========
DATA_JSONL = "/content/drive/MyDrive/Data_Hango/qwen_finetune_dataset.jsonl"
IMAGE_ROOT = "/content/drive/MyDrive/Data_Hango/uploads"
# ================================================

# Kiểm tra tồn tại
assert os.path.exists(DATA_JSONL), f"Không thấy file: {DATA_JSONL}"
assert os.path.exists(IMAGE_ROOT), f"Không thấy thư mục ảnh: {IMAGE_ROOT}"

from datasets import Dataset

def normalize_item(obj):
    """
    Trả về list[sample]; mỗi sample = {'messages': List[dict], 'images': List[str]}
    - Nếu obj['messages'] là List[dict]  -> 1 sample
    - Nếu là List[List[dict]]            -> tách thành nhiều sample (zip với obj['images'])
    """
    out = []
    msgs = obj.get("messages", [])
    imgs = obj.get("images", [])

    # case: nhiều conversation trong 1 dòng
    if msgs and isinstance(msgs[0], list):
        for i, conv in enumerate(msgs):
            imgs_i = imgs[i] if (isinstance(imgs, list) and i < len(imgs)) else []
            if isinstance(imgs_i, str): imgs_i = [imgs_i]
            out.append({"messages": conv, "images": imgs_i})
    # case: 1 conversation chuẩn
    elif msgs and isinstance(msgs[0], dict):
        imgs_ = imgs if (imgs and isinstance(imgs[0], str)) else (imgs[0] if imgs else [])
        if isinstance(imgs_, str): imgs_ = [imgs_]
        out.append({"messages": msgs, "images": imgs_})
    return out

samples = []
with open(DATA_JSONL, "r", encoding="utf-8") as f:
    for li, line in enumerate(f):
        try:
            obj = json.loads(line)
            norm = normalize_item(obj)
            if not norm:
                print(f"⚠️ Skip line {li}: bad 'messages'")
            samples.extend(norm)
        except Exception as e:
            print(f"⚠️ JSON error line {li}: {e}")

assert len(samples) > 0, "Dataset rỗng sau khi chuẩn hoá."

# Tạo HF dataset & split
ds = Dataset.from_list(samples).train_test_split(test_size=0.1, seed=42)
train_ds, eval_ds = ds["train"], ds["test"]
print("Train/Eval sizes:", len(train_ds), len(eval_ds))

# In thử 1 mẫu
for k in ["messages","images"]:
    print(k, "=>", train_ds[0][k][:1] if k=="messages" else train_ds[6][k])

# Print the entire train_ds dataset
print("Printing the entire train_ds dataset:")
for i, example in enumerate(train_ds):
    print(f"Example {i}: {example}")

print("Finished printing train_ds.")

from transformers import (
    Qwen2VLForConditionalGeneration,
    AutoProcessor,
    BitsAndBytesConfig,
)

use_cuda = torch.cuda.is_available()
bnb_config = None

if use_cuda:
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16
    )

model = Qwen2VLForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2-VL-2B-Instruct",
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
)
model.config.use_cache = False  # cần tắt khi training

processor = AutoProcessor.from_pretrained("Qwen/Qwen2-VL-2B-Instruct", trust_remote_code=True)

# Đảm bảo có PAD token
if processor.tokenizer.pad_token is None:
    processor.tokenizer.pad_token = processor.tokenizer.eos_token
processor.tokenizer.padding_side = "right"

print("Model & Processor loaded.")

class QwenVLChatCollator:
    def __init__(self, processor, image_root, max_length=1024):
        self.processor = processor
        self.image_root = image_root
        self.max_length = max_length

    def _load_messages_with_images(self, messages):
        """
        - Chuyển content['image'] từ tên file -> PIL.Image
        - Trả về (messages_mới, list PIL images đúng thứ tự xuất hiện)
        """
        pil_images_in_order = []
        msgs_out = []

        for m in messages:
            role = m.get("role", "user")
            content = m.get("content", [])
            new_c = []
            # content có thể là str hoặc list[dict]
            if isinstance(content, str):
                new_c.append({"type": "text", "text": content})
            elif isinstance(content, list):
                for c in content:
                    if isinstance(c, dict) and c.get("type") == "text":
                        new_c.append({"type": "text", "text": c.get("text", "")})
                    elif isinstance(c, dict) and c.get("type") == "image":
                        name = c.get("image")
                        if name:
                            p = name if os.path.isabs(name) else os.path.join(self.image_root, name)
                            try:
                                im = Image.open(p).convert("RGB")
                                new_c.append({"type": "image", "image": im})
                                pil_images_in_order.append(im)
                            except Exception as e:
                                # nếu ảnh thiếu thì bỏ qua image đó
                                pass
            else:
                # fallback
                new_c.append({"type": "text", "text": str(content)})

            msgs_out.append({"role": role, "content": new_c})

        return msgs_out, pil_images_in_order

    def __call__(self, features):
        texts = []
        images_per_sample = []

        for feat in features:
            msgs = feat["messages"]
            msgs_pil, ordered_imgs = self._load_messages_with_images(msgs)

            # tạo chat template có kèm token <image> theo đúng thứ tự
            text = self.processor.apply_chat_template(
                msgs_pil, tokenize=False, add_generation_prompt=False
            )
            texts.append(text)
            images_per_sample.append(ordered_imgs)  # List[PIL.Image] (có thể rỗng)

        enc = self.processor(
            text=texts,
            images=images_per_sample,     # List[List[PIL.Image]]
            padding=True,
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt",
        )

        # gán nhãn = input_ids, mask pad thành -100
        labels = enc["input_ids"].clone()
        if "attention_mask" in enc:
            labels[enc["attention_mask"] == 0] = -100
        else:
            pad_id = self.processor.tokenizer.pad_token_id
            labels[labels == pad_id] = -100
        enc["labels"] = labels
        return enc

data_collator = QwenVLChatCollator(processor, IMAGE_ROOT, max_length=1024)
print("Collator ready.")

from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

if use_cuda and any(p.dtype == torch.float16 for p in model.parameters()):
    # chuẩn bị cho k-bit training nếu đang dùng 4-bit
    model = prepare_model_for_kbit_training(model)

lora = LoraConfig(
    task_type="CAUSAL_LM",
    r=16, lora_alpha=32, lora_dropout=0.1,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
)
model = get_peft_model(model, lora)

# (tuỳ chọn) giảm VRAM
if hasattr(model, "gradient_checkpointing_enable"):
    model.gradient_checkpointing_enable()

model.print_trainable_parameters()

from transformers import TrainingArguments
from trl import SFTTrainer

OUTPUT_DIR = "/content/drive/MyDrive/qwen2vl-finetuned"

args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    num_train_epochs=1,                 # test nhanh; tăng lên khi train thật
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    learning_rate=1e-4,
    fp16=use_cuda,
    save_steps=50,
    logging_steps=10,
    warmup_steps=20,
    max_grad_norm=1.0,
    weight_decay=0.01,
    optim="adamw_torch",
    remove_unused_columns=False,        # GIỮ 'messages' & 'images'
    report_to="none",
    evaluation_strategy="steps",
    eval_steps=50,
    load_best_model_at_end=False,
    dataloader_pin_memory=False,        # THÊM DÒNG NÀY
    dataloader_num_workers=0,           # THÊM DÒNG NÀY
)

from transformers import Trainer, TrainingArguments
import torch.nn as nn

class QwenVLTrainer(Trainer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
    
    def compute_loss(self, model, inputs, return_outputs=False):
        """
        Custom loss computation for multimodal training
        """
        outputs = model(**inputs)
        loss = outputs.loss if hasattr(outputs, 'loss') and outputs.loss is not None else None
        
        # Fallback loss computation nếu model không tự tính
        if loss is None and "labels" in inputs:
            labels = inputs["labels"]
            logits = outputs.logits
            
            # Shift để predict next token
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            
            # Flatten
            shift_logits = shift_logits.view(-1, shift_logits.size(-1))
            shift_labels = shift_labels.view(-1)
            
            # Cross entropy loss
            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)
            loss = loss_fct(shift_logits, shift_labels)

        return (loss, outputs) if return_outputs else loss

trainer = QwenVLTrainer(
    model=model,
    args=args,
    train_dataset=train_ds,
    eval_dataset=eval_ds,
    tokenizer=processor.tokenizer,
    data_collator=data_collator,
)

print("Custom trainer ready!")
trainer.train()

FINAL_DIR = os.path.join(OUTPUT_DIR, "final")
trainer.save_model(FINAL_DIR)
processor.save_pretrained(FINAL_DIR)
print("Saved to:", FINAL_DIR)

model.eval()

# Lấy 1 mẫu eval
sample = eval_ds[random.randrange(len(eval_ds))]
messages = sample["messages"]

# Chuẩn bị messages (mở ảnh)
msgs_for_gen = []
imgs = []
for m in messages:
    role = m.get("role","user")
    content = []
    for c in m.get("content", []):
        if isinstance(c, dict) and c.get("type") == "text":
            content.append({"type": "text", "text": c.get("text","")})
        elif isinstance(c, dict) and c.get("type") == "image" and c.get("image"):
            p = c["image"] if os.path.isabs(c["image"]) else os.path.join(IMAGE_ROOT, c["image"])
            if os.path.exists(p):
                im = Image.open(p).convert("RGB")
                content.append({"type": "image", "image": im})
                imgs.append(im)
    msgs_for_gen.append({"role": role, "content": content})

# Thêm generation prompt
text = processor.apply_chat_template(msgs_for_gen, tokenize=False, add_generation_prompt=True)

inputs = processor(
    text=[text],
    images=[imgs],   # list of PIL theo thứ tự
    return_tensors="pt"
).to(model.device)

with torch.no_grad():
    out = model.generate(
        **inputs,
        max_new_tokens=256,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
        eos_token_id=processor.tokenizer.eos_token_id,
    )

decoded = processor.tokenizer.decode(out[0], skip_special_tokens=True)
print("=== GENERATED ===")
print(decoded.split("<|assistant|>")[-1].strip())